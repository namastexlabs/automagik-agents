---
description: "Performance patterns, caching strategies, and optimization techniques"
globs:
  - "**/src/**/*.py"
  - "**/src/ai/**"
  - "**/src/memory/**"
  - "**/src/api/**"
alwaysApply: false
priority: 13
---

# Performance Optimization Guide

## AI Service Layer Optimization

### Centralized AI Management

**Inspired by claude-task-master's unified AI service pattern**, implement centralized AI management:

```python
# src/ai/unified_service.py
from typing import Optional, Dict, Any, Literal
from dataclasses import dataclass
import time
import asyncio
from functools import wraps

@dataclass
class TelemetryData:
    timestamp: str
    user_id: str
    command_name: str
    model_used: str
    provider_name: str
    input_tokens: int
    output_tokens: int
    total_tokens: int
    total_cost: float
    currency: str = "USD"

class UnifiedAIService:
    """Centralized AI service with fallback, retry, and telemetry."""
    
    def __init__(self):
        self.providers = {}
        self.fallback_sequence = ['main', 'fallback', 'research']
        self.retry_config = {'max_retries': 3, 'backoff_factor': 2}
        
    async def generate_response(
        self,
        role: Literal['main', 'research', 'fallback'],
        system_prompt: str,
        user_prompt: str,
        session_context: Optional[Dict] = None,
        command_name: str = "unknown"
    ) -> Dict[str, Any]:
        """Generate AI response with telemetry and fallback."""
        
        start_time = time.time()
        telemetry_data = None
        
        for provider_role in self._get_fallback_sequence(role):
            try:
                result = await self._attempt_provider_call(
                    provider_role, system_prompt, user_prompt, session_context
                )
                
                telemetry_data = self._create_telemetry(
                    command_name, provider_role, result, start_time
                )
                
                return {
                    'response': result['content'],
                    'telemetry_data': telemetry_data,
                    'provider_used': provider_role
                }
                
            except Exception as e:
                if provider_role == self.fallback_sequence[-1]:
                    raise e
                continue
                
        raise RuntimeError("All AI providers failed")
    
    def _get_fallback_sequence(self, role: str) -> List[str]:
        """Get provider fallback sequence starting with requested role."""
        sequence = [role]
        for provider in self.fallback_sequence:
            if provider != role:
                sequence.append(provider)
        return sequence
```

### AI Response Caching

```python
# src/ai/cache.py
from functools import lru_cache
import hashlib
import json
from typing import Dict, Any, Optional

class AIResponseCache:
    """LRU cache for AI responses with intelligent invalidation."""
    
    def __init__(self, max_size: int = 1000):
        self.cache = {}
        self.max_size = max_size
        self.access_times = {}
    
    def _generate_cache_key(
        self, 
        system_prompt: str, 
        user_prompt: str, 
        model: str,
        context_hash: Optional[str] = None
    ) -> str:
        """Generate unique cache key for AI request."""
        content = f"{system_prompt}|{user_prompt}|{model}"
        if context_hash:
            content += f"|{context_hash}"
        return hashlib.sha256(content.encode()).hexdigest()[:16]
    
    async def get_or_generate(
        self,
        cache_key: str,
        generator_func,
        max_age_seconds: int = 3600
    ) -> Dict[str, Any]:
        """Get cached response or generate new one."""
        
        if cache_key in self.cache:
            cached_entry = self.cache[cache_key]
            if time.time() - cached_entry['timestamp'] < max_age_seconds:
                return {
                    **cached_entry['response'],
                    'from_cache': True
                }
        
        # Generate new response
        response = await generator_func()
        
        # Cache the response
        self.cache[cache_key] = {
            'response': response,
            'timestamp': time.time()
        }
        
        # Evict old entries if needed
        if len(self.cache) > self.max_size:
            self._evict_oldest()
        
        return {
            **response,
            'from_cache': False
        }
```

## Memory System Performance

### Connection Pooling

```python
# src/memory/connection_pool.py
import asyncio
import asyncpg
from typing import Dict, Any
from contextlib import asynccontextmanager

class MemoryConnectionPool:
    """Optimized PostgreSQL connection pool for memory operations."""
    
    def __init__(self, database_url: str, min_connections: int = 5, max_connections: int = 20):
        self.database_url = database_url
        self.min_connections = min_connections
        self.max_connections = max_connections
        self._pool = None
    
    async def initialize(self):
        """Initialize connection pool."""
        self._pool = await asyncpg.create_pool(
            self.database_url,
            min_size=self.min_connections,
            max_size=self.max_connections,
            command_timeout=60
        )
    
    @asynccontextmanager
    async def acquire(self):
        """Acquire connection from pool."""
        async with self._pool.acquire() as conn:
            yield conn
    
    async def execute_optimized_query(
        self,
        query: str,
        *args,
        timeout: int = 30
    ) -> Any:
        """Execute query with performance optimization."""
        async with self.acquire() as conn:
            # Enable query plan caching
            await conn.execute("SET plan_cache_mode = force_generic_plan")
            return await conn.fetch(query, *args, timeout=timeout)
```

### Memory Template Caching

```python
# src/memory/template_cache.py
from typing import Dict, Set
import re
from functools import lru_cache

class TemplateCache:
    """Cache compiled memory templates for performance."""
    
    def __init__(self):
        self._compiled_templates = {}
        self._variable_patterns = {}
    
    @lru_cache(maxsize=500)
    def compile_template(self, template: str) -> Dict[str, Any]:
        """Compile template and cache variable patterns."""
        
        # Find all {{variable}} patterns
        variable_pattern = re.compile(r'\{\{(\w+)\}\}')
        variables = variable_pattern.findall(template)
        
        # Create optimized replacement function
        def replace_variables(context: Dict[str, str]) -> str:
            result = template
            for var in variables:
                if var in context:
                    result = result.replace(f'{{{{{var}}}}}', context[var])
            return result
        
        return {
            'variables': set(variables),
            'replacer': replace_variables,
            'template': template
        }
    
    def render_template(
        self, 
        template: str, 
        context: Dict[str, str]
    ) -> str:
        """Render template with context using cached compilation."""
        compiled = self.compile_template(template)
        return compiled['replacer'](mdc:context)
```

## API Performance Optimization

### Response Compression & Caching

```python
# src/api/middleware/performance.py
from fastapi import Request, Response
from fastapi.responses import JSONResponse
import gzip
import json
from typing import Dict, Any
import time

class PerformanceMiddleware:
    """Middleware for API performance optimization."""
    
    def __init__(self):
        self.response_cache = {}
        self.cache_ttl = 300  # 5 minutes
    
    async def __call__(self, request: Request, call_next):
        start_time = time.time()
        
        # Check cache for GET requests
        if request.method == "GET":
            cache_key = self._generate_cache_key(request)
            cached_response = self._get_cached_response(cache_key)
            if cached_response:
                return cached_response
        
        # Process request
        response = await call_next(request)
        
        # Add performance headers
        process_time = time.time() - start_time
        response.headers["X-Process-Time"] = str(process_time)
        
        # Compress large responses
        if hasattr(response, 'body') and len(response.body) > 1024:
            response = self._compress_response(response)
        
        # Cache GET responses
        if request.method == "GET" and response.status_code == 200:
            self._cache_response(cache_key, response)
        
        return response
    
    def _compress_response(self, response: Response) -> Response:
        """Compress response body if large."""
        if isinstance(response, JSONResponse):
            content = json.dumps(response.body)
            compressed = gzip.compress(content.encode())
            
            if len(compressed) < len(content):
                response.headers["Content-Encoding"] = "gzip"
                response.body = compressed
        
        return response
```

### Database Query Optimization

```python
# src/api/database/optimized_queries.py
from typing import List, Dict, Any, Optional
import asyncpg

class OptimizedQueries:
    """Optimized database queries for common operations."""
    
    def __init__(self, pool):
        self.pool = pool
    
    async def get_agent_memory_optimized(
        self,
        agent_id: str,
        session_name: str,
        limit: int = 50
    ) -> List[Dict]:
        """Optimized memory retrieval with minimal data transfer."""
        
        query = """
        SELECT name, content, created_at
        FROM agent_memory 
        WHERE agent_id = $1 AND session_name = $2
        ORDER BY created_at DESC
        LIMIT $3
        """
        
        async with self.pool.acquire() as conn:
            # Use prepared statements for better performance
            stmt = await conn.prepare(query)
            rows = await stmt.fetch(agent_id, session_name, limit)
            
            return [dict(row) for row in rows]
    
    async def batch_update_agent_status(
        self,
        agent_updates: List[Dict[str, Any]]
    ) -> int:
        """Batch update multiple agents for better performance."""
        
        if not agent_updates:
            return 0
        
        query = """
        UPDATE agents 
        SET status = $2, updated_at = NOW()
        WHERE id = $1
        """
        
        async with self.pool.acquire() as conn:
            async with conn.transaction():
                result = await conn.executemany(
                    query,
                    [(update['id'], update['status']) for update in agent_updates]
                )
                return len(agent_updates)
```

## Agent Performance Patterns

### Asynchronous Processing

```python
# src/agents/performance/async_handler.py
import asyncio
from typing import List, Dict, Any, Callable
from concurrent.futures import ThreadPoolExecutor
import time

class AsyncAgentHandler:
    """Handle multiple agent operations asynchronously."""
    
    def __init__(self, max_workers: int = 10):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.active_operations = {}
    
    async def process_multiple_messages(
        self,
        agent_requests: List[Dict[str, Any]],
        timeout: int = 30
    ) -> List[Dict[str, Any]]:
        """Process multiple agent messages concurrently."""
        
        tasks = []
        for request in agent_requests:
            task = asyncio.create_task(
                self._process_single_message(request)
            )
            tasks.append(task)
        
        # Wait for all tasks with timeout
        try:
            results = await asyncio.wait_for(
                asyncio.gather(*tasks, return_exceptions=True),
                timeout=timeout
            )
            
            return [
                {'success': True, 'data': result} if not isinstance(result, Exception)
                else {'success': False, 'error': str(result)}
                for result in results
            ]
            
        except asyncio.TimeoutError:
            # Cancel remaining tasks
            for task in tasks:
                if not task.done():
                    task.cancel()
            
            raise RuntimeError(f"Agent processing timeout after {timeout}s")
    
    async def _process_single_message(self, request: Dict[str, Any]) -> str:
        """Process single agent message."""
        agent = request['agent']
        message = request['message']
        session = request.get('session', 'default')
        
        return await agent.process_message(message, session)
```

### Resource Monitoring

```python
# src/monitoring/performance_monitor.py
import psutil
import time
import asyncio
from typing import Dict, Any
from dataclasses import dataclass, asdict

@dataclass
class PerformanceMetrics:
    timestamp: float
    cpu_percent: float
    memory_percent: float
    memory_used_mb: float
    active_connections: int
    response_time_ms: float
    cache_hit_rate: float
    ai_calls_per_minute: int

class PerformanceMonitor:
    """Monitor system and application performance."""
    
    def __init__(self):
        self.metrics_history = []
        self.max_history = 1000
        self.alert_thresholds = {
            'cpu_percent': 80.0,
            'memory_percent': 85.0,
            'response_time_ms': 5000.0
        }
    
    async def collect_metrics(self) -> PerformanceMetrics:
        """Collect current performance metrics."""
        
        process = psutil.Process()
        memory_info = process.memory_info()
        
        metrics = PerformanceMetrics(
            timestamp=time.time(),
            cpu_percent=process.cpu_percent(),
            memory_percent=process.memory_percent(),
            memory_used_mb=memory_info.rss / 1024 / 1024,
            active_connections=self._get_active_connections(),
            response_time_ms=self._get_avg_response_time(),
            cache_hit_rate=self._get_cache_hit_rate(),
            ai_calls_per_minute=self._get_ai_calls_per_minute()
        )
        
        self._store_metrics(metrics)
        self._check_alerts(metrics)
        
        return metrics
    
    def _check_alerts(self, metrics: PerformanceMetrics):
        """Check if any metrics exceed alert thresholds."""
        alerts = []
        
        for metric, threshold in self.alert_thresholds.items():
            value = getattr(metrics, metric)
            if value > threshold:
                alerts.append(f"{metric}: {value} > {threshold}")
        
        if alerts:
            # Log alerts or send notifications
            print(f"Performance alerts: {', '.join(alerts)}")
```

## Error Handling & Recovery

### Circuit Breaker Pattern

```python
# src/utils/circuit_breaker.py
import time
import asyncio
from enum import Enum
from typing import Callable, Any, Optional

class CircuitState(Enum):
    CLOSED = "closed"
    OPEN = "open"
    HALF_OPEN = "half_open"

class CircuitBreaker:
    """Circuit breaker for external service calls."""
    
    def __init__(
        self,
        failure_threshold: int = 5,
        reset_timeout: int = 60,
        expected_exception: type = Exception
    ):
        self.failure_threshold = failure_threshold
        self.reset_timeout = reset_timeout
        self.expected_exception = expected_exception
        self.failure_count = 0
        self.last_failure_time = None
        self.state = CircuitState.CLOSED
    
    async def call(self, func: Callable, *args, **kwargs) -> Any:
        """Execute function with circuit breaker protection."""
        
        if self.state == CircuitState.OPEN:
            if self._should_attempt_reset():
                self.state = CircuitState.HALF_OPEN
            else:
                raise RuntimeError("Circuit breaker is OPEN")
        
        try:
            result = await func(*args, **kwargs)
            self._on_success()
            return result
            
        except self.expected_exception as e:
            self._on_failure()
            raise e
    
    def _on_success(self):
        """Handle successful call."""
        self.failure_count = 0
        self.state = CircuitState.CLOSED
    
    def _on_failure(self):
        """Handle failed call."""
        self.failure_count += 1
        self.last_failure_time = time.time()
        
        if self.failure_count >= self.failure_threshold:
            self.state = CircuitState.OPEN
    
    def _should_attempt_reset(self) -> bool:
        """Check if circuit should attempt reset."""
        return (
            self.last_failure_time and
            time.time() - self.last_failure_time >= self.reset_timeout
        )
```

## Configuration & Monitoring

### Performance Configuration

```python
# src/config/performance.py
from pydantic import BaseSettings
from typing import Dict, Any

class PerformanceSettings(BaseSettings):
    """Performance-related configuration."""
    
    # AI Service
    ai_cache_size: int = 1000
    ai_cache_ttl: int = 3600
    ai_timeout: int = 30
    ai_max_retries: int = 3
    
    # Database
    db_pool_min_size: int = 5
    db_pool_max_size: int = 20
    db_query_timeout: int = 30
    
    # API
    api_response_cache_ttl: int = 300
    api_compression_threshold: int = 1024
    api_rate_limit: int = 100
    
    # Memory
    memory_template_cache_size: int = 500
    memory_batch_size: int = 50
    
    # Monitoring
    metrics_collection_interval: int = 60
    performance_log_level: str = "INFO"
    
    class Config:
        env_prefix = "PERF_"
        env_file = ".env"

# Usage
performance_config = PerformanceSettings()
```

## Quality Gates for Performance

### Performance Testing Requirements

```python
# tests/performance/test_performance.py
import pytest
import time
import asyncio
from src.agents.models.automagik_agent import AutomagikAgent

class TestPerformanceRequirements:
    """Performance quality gates."""
    
    @pytest.mark.asyncio
    async def test_agent_response_time(self):
        """Agent responses must be under 5 seconds."""
        agent = TestAgent()
        
        start_time = time.time()
        response = await agent.process_message("test message", "test_session")
        end_time = time.time()
        
        response_time = end_time - start_time
        assert response_time < 5.0, f"Response time {response_time}s exceeds 5s limit"
    
    @pytest.mark.asyncio
    async def test_memory_retrieval_performance(self):
        """Memory retrieval must be under 1 second."""
        from src.memory.connection_pool import MemoryConnectionPool
        
        pool = MemoryConnectionPool("test_db_url")
        await pool.initialize()
        
        start_time = time.time()
        memories = await pool.execute_optimized_query(
            "SELECT * FROM agent_memory WHERE agent_id = $1 LIMIT 50",
            "test_agent"
        )
        end_time = time.time()
        
        query_time = end_time - start_time
        assert query_time < 1.0, f"Memory query time {query_time}s exceeds 1s limit"
    
    def test_api_concurrent_requests(self):
        """API must handle 100 concurrent requests."""
        # Implementation for load testing
        pass
```

## Implementation Checklist

**Before deploying performance optimizations:**

- [ ] **AI Service Optimization**: Centralized service with fallback and caching
- [ ] **Memory Performance**: Connection pooling and template caching
- [ ] **API Optimization**: Compression, caching, and async processing
- [ ] **Monitoring**: Performance metrics collection and alerting
- [ ] **Error Handling**: Circuit breakers and graceful degradation
- [ ] **Quality Gates**: Performance tests with specific time limits
- [ ] **Configuration**: Tunable performance parameters
- [ ] **Resource Monitoring**: CPU, memory, and connection tracking

---

**Remember**: Performance optimization should be data-driven. Monitor metrics, identify bottlenecks, and optimize the most impactful areas first. The patterns above provide a foundation for scalable, high-performance automagik-agents deployments.
